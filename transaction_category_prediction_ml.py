# -*- coding: utf-8 -*-
"""Transaction Category Prediction ML

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XF_ccRuwJZ_rrfbsI_8CAdmy19XEqUi8
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("vipin20/transaction-data")

print("Path to dataset files:", path)

import pandas as pd
# Uploading the csv file
df=pd.read_csv(path+"/transaction_data.csv");
df.head()

import re
from sklearn.preprocessing import LabelEncoder, StandardScaler,MinMaxScaler
import time
import numpy as np
from scipy import sparse
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import MiniBatchKMeans, KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score
# checking the duplicates and removing them
df.info()

df['UserId'] = df['UserId'].replace(-1,1)  # keep your logic exactly as it is
df['TotalCost'] = df['NumberOfItemsPurchased'] * df['CostPerItem']

def clear_Description(description):
    description = str(description).lower()
    description = re.sub(r'[^a-zA-Z0-9 ]+', ' ', description)#ChatGPT assessted
    description = re.sub(r'\s+', ' ', description).strip()#ChatGPT assessted
    return description

df['Description'] = df['ItemDescription'].astype(str).apply(clear_Description)#ChatGPT assessted

# drop duplicates
df_new = df.drop_duplicates()

df_new = df_new.drop(columns=["ItemDescription","TransactionId"])

print('-----------------without duplicates-------------------')
df_new.info()

print('-----------------number of nulls Before----------------')
print(df_new.isna().sum())
print('-----------------number of nulls After----------------')
print(df_new.isna().sum())

# label encode Country
# le_country = LabelEncoder()
# df_new['Country'] = le_country.fit_transform(df_new['Country'])
# df_new['Country'] = df_new['Country'].astype(int)
# df_new.info()

# EDA

# country_idx_to_name = {i: name for i, name in enumerate(le_country.classes_)}
# df_new['Country'] = df_new['Country'].map(country_idx_to_name)
#ChatGPT assessted
plt.figure(figsize=(10,5))
top_countries = df_new['Country'].value_counts().head(10)
sns.barplot(x=top_countries.index, y=top_countries.values)
plt.title("Top 10 Countries by number of transactions")
plt.xticks(rotation=45, ha='right')
plt.ylabel("Count")
plt.xlabel("Country")
plt.tight_layout()
plt.show()

# Boxplots
top_countries = df_new["Country"].value_counts().head(10).index
df_top = df_new[df_new["Country"].isin(top_countries)]

# TotalCost vs Country
plt.figure(figsize=(12,5))
sns.boxplot(x="Country", y="TotalCost", data=df_top)
plt.xticks(rotation=45)
plt.title("TotalCost by Country")
plt.show()

# NumberOfItemsPurchased vs Country
plt.figure(figsize=(12,5))
sns.boxplot(x="Country", y="NumberOfItemsPurchased", data=df_top)
plt.xticks(rotation=45)
plt.title("Items Purchased by Country")
plt.show()

# CostPerItem vs Country
plt.figure(figsize=(12,5))
sns.boxplot(x="Country", y="CostPerItem", data=df_top)
plt.xticks(rotation=45)
plt.title("Cost Per Item by Country")
plt.show()

# Remove outliers using IQR
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return df[(df[column] >= lower) & (df[column] <= upper)]

cols = ["TotalCost", "NumberOfItemsPurchased", "CostPerItem"]

for col in cols:
    df_new = remove_outliers(df_new, col)

# Boxplots
top_countries = df_new["Country"].value_counts().head(10).index
df_top = df_new[df_new["Country"].isin(top_countries)]

# TotalCost vs Country
plt.figure(figsize=(12,5))
sns.boxplot(x="Country", y="TotalCost", data=df_top)
plt.xticks(rotation=45)
plt.title("TotalCost by Country")
plt.show()

# NumberOfItemsPurchased vs Country
plt.figure(figsize=(12,5))
sns.boxplot(x="Country", y="NumberOfItemsPurchased", data=df_top)
plt.xticks(rotation=45)
plt.title("Items Purchased by Country")
plt.show()

# CostPerItem vs Country
plt.figure(figsize=(12,5))
sns.boxplot(x="Country", y="CostPerItem", data=df_top)
plt.xticks(rotation=45)
plt.title("Cost Per Item by Country")
plt.show()

# I dont need the country to avoid the overfitting
df_new = df_new.drop(columns=["Country"])

# feature engineering section
colums = ['TotalCost', 'NumberOfItemsPurchased', 'CostPerItem']
X_before= df_new[colums]
scaler = StandardScaler()
X_after = scaler.fit_transform(X_before)
print('------------------------Scaler----------------------------')
print(X_after)
normalization=MinMaxScaler()
X_after_norm=normalization.fit_transform(X_before)
print('---------------------Normalization------------------------')
print(X_after_norm)

#ChatGPT assessted
N = df_new.shape[0]
sample_size_for_k = 40000
sample_idx = np.random.RandomState(42).choice(N, size=min(sample_size_for_k, N), replace=False)#ChatGPT assessted

tf = TfidfVectorizer(stop_words='english',
                     max_features=5000,
                     ngram_range=(1,2))

X_text = tf.fit_transform(df_new['Description'].astype(str))
num_cols = ['TotalCost', 'NumberOfItemsPurchased', 'CostPerItem']
# X_sparse = sparse.csr_matrix(X_after)

# To compare normalization path
X_sparse = sparse.csr_matrix(X_after_norm)
X_combined = hstack([X_text, X_sparse], format='csr')

# SVD reduction

svd = TruncatedSVD(n_components=150, random_state=42)

X_reduced = svd.fit_transform(X_combined)

#K-search
X_reduced_sample = X_reduced[sample_idx]

Ks = list(range(10, 60))
inertias = []
silhouettes = []

for k in Ks:
    km = KMeans(n_clusters=k, random_state=42, max_iter=100, n_init=3)
    labels = km.fit_predict(X_reduced_sample)
    inertias.append(km.inertia_)
    # silhouette
    sil = silhouette_score(X_reduced_sample, labels, metric='euclidean', sample_size=10000, random_state=42)
    silhouettes.append(sil)
    print(f"k={k}, silhouette={sil:.4f}")

# Plot results
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(Ks, inertias, marker='o')
plt.title("Elbow (inertia) vs k")
plt.xlabel("k")
plt.ylabel("Inertia")
plt.grid(True)

plt.subplot(1,2,2)
plt.plot(Ks, silhouettes, marker='o')
plt.title("Silhouette vs k")
plt.xlabel("k")
plt.ylabel("Silhouette Score")
plt.grid(True)

plt.tight_layout()
plt.show()

print("Silhouette scores:", silhouettes)

from sklearn.cluster import KMeans
# based on silhouette there are several points but 56 the best fit and in elbow there is no elbow
K = 56

kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_reduced)


df_new['Cluster'] = labels
print(labels)

#ChatGPT assessted
for c in range(K):
    print(f"\n====== CLUSTER {c} ======")
    cluster_df = df_new[df_new['Cluster'] == c]

    if (len(cluster_df)<8):
      n=len(cluster_df)
    else:
      n=8

    sample = cluster_df.sample(n, random_state=42)
    display(sample[['Description','TotalCost','NumberOfItemsPurchased']])

# since we have the K (number of clusters) lets categorize
cluster_to_category = {
  0:"Lunch Bags (Skull / Charlotte style)",
  1:"Mixed Small Gifts (misc)",
  2:"Home Sweet Home / Hooks & Small Home Decor",
  3:"Hanging Heart / Tea-light Holders",
  4:"Paper Chain Kits / Craft Kits",
  5:"Jumbo Storage Bags",
  6:"Travel Card Wallets / Small Pouches",
  7:"Hot Water Bottles",
  8:"Jumbo / Charlotte Bags (Apples design)",
  9:"Union Jack / Flag Homeware",
  10:"Wicker Hearts (decor)",
  11:"Red Retrospot Collection",
  12:"Alarm Clocks & Clocks",
  13:"Suki / Charlotte Lunch Bags",
  14:"Regency Tea Cup & Saucers",
  15:"Vintage Leaf / Doily Bags",
  16:"Seasonal Decorations / Ornaments",
  17:"Blue / Polkadot Home Textiles",
  18:"Drawer Knobs / Cabinet Hardware",
  19:"Boxes, Storage & Recipe Boxes",
  20:"Metal Signs",
  21:"Pantry / Kitchen Sets & Doilies",
  22:"Popcorn Holders",
  23:"Cake Cases (60-set)",
  24:"Alphabet Lunch / Jumbo Bags",
  25:"Pink Polkadot Collection",
  26:"Regency Cake Stands",
  27:"Hanging / Antique Tea-lights",
  28:"Plasters / First Aid (tins)",
  29:"Christmas wrap / crackers",
  30:"Gardeners Kneeling Pads",
  31:"Heart Decorations / Zinc Heart",
  32:"Building Block Words (home/bath)",
  33:"Assorted Bird Ornaments",
  34:"No Description / NAN bucket",
  35:"Spaceboy / Themed Lunch Bags",
  36:"Vintage-style Misc",
  37:"Postage / Dotcom Postage",
  38:"Natural Slate Chalkboards",
  39:"Round Snack Boxes / Food",
  40:"Wooden Homeware & Frames",
  41:"Dolly Girl Collection",
  42:"Small Packs (tissues/pencils)",
  43:"Party Bunting",
  44:"French Blue Metal Door Signs",
  45:"Kitchen Scales",
  46:"Night Lights",
  47:"Cake Tin sets (3-piece)",
  48:"Stationery & Craft Sets",
  49:"Jumbo Bags (various)",
  50:"Ribbon Reels / Craft Ribbon",
  51:"Wall Art / Decorative Signs",
  52:"Jam Making Sets",
  53:"Pack of 72 Cake Cases",
  54:"Photo Frames",
  55:"Feltcraft / Soft Craft Kits"
}


df_new['Category'] = df_new['Cluster'].map(cluster_to_category).fillna("Unlabeled")
print(df_new['Category'].unique())

X_T = df_new['Description'].astype(str)
X_N = df_new[['TotalCost','NumberOfItemsPurchased','CostPerItem']].astype(float)
y = df_new['Category']

X_Ttrain, X_Ttest, X_Ntrain, X_Ntest, y_train, y_test = train_test_split(
    X_T, X_N, y, test_size=0.20, stratify=y, random_state=42
)

X_Ttrain_tf = tf.fit_transform(X_Ttrain)
X_Ttest_tf = tf.transform(X_Ttest)

X_Ttrain_svd = svd.fit_transform(X_Ttrain_tf)
X_Ttest_svd = svd.transform(X_Ttest_tf)

X_Ntrain_scaled = scaler.fit_transform(X_Ntrain)
X_Ntest_scaled = scaler.transform(X_Ntest)


X_train= np.hstack([X_Ttrain_svd, X_Ntrain_scaled])
X_test= np.hstack([X_Ttest_svd, X_Ntest_scaled])

model = LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred, zero_division=0))

cm = confusion_matrix(y_test, y_pred, labels=model.classes_)

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

y_train_shuffled = np.random.permutation(y_train)#ChatGPT assessted

model.fit(X_train, y_train_shuffled)

y_pred_shuffled = model.predict(X_test)
acc = accuracy_score(y_test, y_pred_shuffled)
acc

# without NLP
X_num = df_new[['TotalCost','NumberOfItemsPurchased','CostPerItem']].astype(float)
y = df_new['Category']

X_train_num, X_test_num, y_train_num, y_test_num = train_test_split(
    X_num, y, test_size=0.20, stratify=y, random_state=42
)

X_Ntrain_scaled = scaler.fit_transform(X_train_num)
X_Ntest_scaled = scaler.transform(X_test_num)

model.fit(X_Ntrain_scaled, y_train_num)
accuracy = model.score(X_Ntest_scaled, y_test_num)
print(accuracy)

#ChatGPT assessted
X2 = X_reduced[:, :2]
labels = df_new['Cluster'].values

plt.figure(figsize=(13, 11))

for c in range(K):
    cluster_points = X2[labels == c]
    centroid = cluster_points.mean(axis=0)
    distances = np.linalg.norm(cluster_points - centroid, axis=1)
    idx = distances.argsort()[:8]
    close_points = cluster_points[idx]
    plt.scatter(close_points[:,0], close_points[:,1], s=50, label=f"Cluster {c}")

plt.title("Closest 8 Points for Each Cluster")
plt.xlabel("1")
plt.ylabel("2")
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=8)
plt.tight_layout()
plt.show()

def user_spending(user_id, df=df_new):
    res = df[df['UserId'] == user_id]
    if res.empty:
        return 'no user found with this id'
    out = res.groupby('Category', as_index=False)['TotalCost'].sum().sort_values('TotalCost', ascending=False)
    out['percenrage'] = (out['TotalCost'] / out['TotalCost'].sum())*100
    return out.head(10000000000000000000000000000000000000000000000000000000000)

# random user
print(user_spending(278166))